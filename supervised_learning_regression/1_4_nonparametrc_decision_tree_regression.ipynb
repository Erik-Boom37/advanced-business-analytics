{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- One of the most widely used and practical methods for inductive inference\n",
    "- Approximates discrete-valued functions (including disjunctions)\n",
    "- Can be used for classification or regression since accept any variable.\n",
    "- Are attractive models if we care about interpretability. Like the name decision tree suggests, we can think of this model as breaking down our data by making decisions based on asking a series of questions\n",
    "- Unlike linear models, decision trees are able to capture non-linear interaction between the features and the target.\n",
    "- In classification is used Information Gain (IG) function, in regression we use MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Decision Tree Example](../img/reg15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Tree in a Regression Problem\n",
    "\n",
    "When predicting a numeric variable, the idea of a tree construction remains the same, but the quality criteria changes: \n",
    "\n",
    "- Variance around the mean (or variance reduction): \n",
    "\n",
    "$$\\Large D = \\frac{1}{\\ell} \\sum\\limits_{i =1}^{\\ell} (y_i) $$\n",
    "\n",
    "where $\\ell$ is the number of samples in a leaf, $y_i$ is the value of the target variable. Simply put, by minimizing the variance around the mean, we look for features that divide the training set in such a way that the values of the target feature in each leaf are roughly equal.\n",
    "\n",
    "- Each internal node represents a test on an feature resulting on the split of the current sample.\n",
    "- At each step the algorithm selects the feature and a cutoff value that maximises a given metric.\n",
    "- Different metrics exist for regression tree (target is continuous) or classification tree (the target is qualitative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note:** is that tree based models are not designed to work with very sparse features. When dealing with sparse input data (e.g. categorical features with large dimension), we can either pre-process the sparse features to generate numerical statistics, or switch to a linear model, which is better suited for such scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Code Dictionary\n",
    "code | description\n",
    "-----|------------\n",
    "`.iloc` | Pandas purely integer-location based indexing for selection by position.\n",
    "`.DecisionTreeRegressor` | Regression model with Decision Trees.\n",
    "`.arrange()` | Generates evenly spaced values within a given interval.\n",
    "`.reshape()` | Gives a new shape to an array without changing its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../datasets/position_salaries.csv')\n",
    "X = dataset.iloc[:, 1:2].values\n",
    "y = dataset.iloc[:, 2].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state = 0)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_grid = np.arange(min(X), max(X), 0.01) \n",
    "X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "plt.scatter(X, y, color = 'red')\n",
    "plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\n",
    "plt.title('Decision Tree')\n",
    "plt.xlabel('Position level')\n",
    "plt.ylabel('Salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Why chose decision trees?\n",
    "Different kinds of models have different advantages. The decision tree model is very good at handling tabular data with numerical features, or categorical features with fewer than hundreds of categories. Unlike linear models, decision trees are able to capture non-linear interaction between the features and the target."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
